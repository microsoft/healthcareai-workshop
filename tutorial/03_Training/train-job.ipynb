{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Azure ML\n",
    "\n",
    "Azure Machine Learning (Azure ML) provides a robust platform for training machine learning models at scale. This notebook guides you through the process of configuring and submitting a training job to Azure ML's managed compute resources, allowing you to leverage powerful GPU instances without managing the underlying infrastructure.\n",
    "\n",
    "This notebook demonstrates how to submit a training job to Azure ML using the Python SDK. The job will:\n",
    "- Train a pneumonia detection model using PyTorch\n",
    "- Run on GPU compute for accelerated training\n",
    "- Leverage the registered RSNA pneumonia dataset\n",
    "- Use custom training parameters (epochs, learning rate)\n",
    "- Track metrics and artifacts automatically with MLflow\n",
    "\n",
    "By defining your training as an Azure ML job, you gain several advantages over running locally. Your experiments are automatically tracked, resources are provisioned on-demand, and your training can scale to multiple GPUs if needed. The job definition includes everything needed to reproduce your training, from code and data to environment configuration and compute resources.\n",
    "\n",
    "Once your job is submitted, you can monitor its progress through logs, metrics, and the Azure ML Studio interface. After training completes, the resulting model will be registered and can be deployed for inference or used in subsequent workflows.\n",
    "\n",
    "## Setup Pre-requisites\n",
    "\n",
    "Before starting, ensure you have the following ready:\n",
    "\n",
    "* You need access to an Azure ML workspace with appropriate permissions.\n",
    "* The RSNA pneumonia detection dataset should already be registered in your workspace.\n",
    "* Your Azure ML workspace should have quota for GPU compute resources.\n",
    "\n",
    "This notebook assumes you've already completed the data preparation steps and have registered your dataset with Azure ML. We'll be using that registered dataset as input to our training job, making it available to our compute cluster during training.\n",
    "\n",
    "## What you will do:\n",
    "\n",
    "* Connect to your Azure ML workspace using the Python SDK.\n",
    "* Set up GPU compute resources for model training.\n",
    "* Create and register a GPU-enabled environment with PyTorch.\n",
    "* Retrieve the RSNA pneumonia detection dataset from your workspace.\n",
    "* Configure and submit a distributed training job to Azure ML.\n",
    "* Monitor the training job's progress through the Azure ML Studio.\n",
    "\n",
    "The Azure ML Python SDK v2 simplifies these tasks through a streamlined interface, allowing you to focus on model development rather than infrastructure management. Throughout this notebook, you'll learn how to use key SDK components to orchestrate your training workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from workshop_helpers.utils import get_unique_name\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential)\n",
    "\n",
    "unique_name = get_unique_name(credential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Compute Target Setup\n",
    "\n",
    "This cell configures a dedicated GPU compute environment for our machine learning training jobs. GPU acceleration is essential for efficiently training deep learning models, especially for computer vision tasks that require significant computational power. By establishing this compute target, we ensure our training pipeline has access to the necessary GPU resources without manually managing infrastructure.\n",
    "\n",
    "The code performs a critical infrastructure step that:\n",
    "\n",
    "- **Automates resource provisioning**: Eliminates the need for manual cluster creation through the Azure portal\n",
    "- **Enables scalability**: Configures auto-scaling to optimize resource usage during training\n",
    "- **Standardizes the environment**: Ensures all training jobs run on consistent, reproducible hardware\n",
    "- **Supports cost management**: Sets minimum instances to zero to avoid charges when the cluster is idle\n",
    "\n",
    "Steps:\n",
    "- Check if a GPU compute cluster named \"gpucluster\" already exists\n",
    "- If not found, create a new compute target with:\n",
    "  - GPU-enabled VM size (Standard_NC6s_v3)\n",
    "  - Auto-scaling configuration (0 minimum to 4 maximum instances)\n",
    "- Wait for cluster creation to complete\n",
    "- Log status of the compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "gpu_cluster_name = f\"gpucluster-{unique_name}\" ## TODO Add unique name\n",
    "try:\n",
    "    compute_target = ml_client.compute.get(gpu_cluster_name)\n",
    "    print(f\"Using existing GPU compute cluster: {gpu_cluster_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating new GPU compute cluster: {gpu_cluster_name}\")\n",
    "    compute_target = AmlCompute(\n",
    "        name=gpu_cluster_name,\n",
    "        size=\"Standard_NC6s_v3\",  # GPU-enabled VM size, adjust if needed\n",
    "        min_instances=0,\n",
    "        max_instances=5,\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute_target).result()\n",
    "    print(f\"Created GPU compute cluster: {gpu_cluster_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Environment Configuration\n",
    "\n",
    "This cell defines and registers a specialized GPU-enabled environment that will be used to execute our training code in the cloud. The environment configuration is a critical component of our machine learning workflow as it ensures all dependencies, frameworks, and GPU drivers are consistently available across training runs.\n",
    "\n",
    "By creating a dedicated environment with PyTorch and CUDA support, we establish a reproducible foundation that:\n",
    "\n",
    "- **Guarantees compatibility**: The curated Docker image provides pre-tested PyTorch with CUDA drivers that are optimized for deep learning\n",
    "- **Maintains consistency**: Environment versioning ensures all experiments run with identical dependencies\n",
    "- **Enables reproducibility**: The combination of the base image and conda file creates a fully documented, reusable environment\n",
    "- **Optimizes performance**: The environment is specifically configured to leverage GPU acceleration for faster model training\n",
    "\n",
    "Steps:\n",
    "- Define a GPU-enabled environment using a PyTorch/CUDA Docker image\n",
    "- Incorporate additional dependencies from a conda YAML file\n",
    "- Add metadata tags for environment categorization\n",
    "- Register the environment in the Azure ML workspace\n",
    "- Capture the environment name and version for reference in training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# This assumes you have an 'environment.yaml' in your code folder that defines your Conda dependencies.\n",
    "gpu_environment = Environment(\n",
    "    name=f\"uw-workshop-gpu-env-{unique_name}\",\n",
    "    description=\"GPU enabled environment\",\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest\",\n",
    "    conda_file=\"src/environment.yml\",\n",
    "    tags={\"gpu\": \"true\"}\n",
    ")\n",
    "\n",
    "# Register or update the environment in your workspace.\n",
    "registered_env = ml_client.environments.create_or_update(gpu_environment)\n",
    "print(f\"Registered environment: {registered_env.name}:{registered_env.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Retrieval\n",
    "\n",
    "This cell accesses our previously registered pneumonia dataset from the Azure ML workspace, ensuring our training job can reference the correct data assets. By retrieving the latest version of the dataset, we establish a connection to the centralized data resource that maintains consistency across all training runs and enables reproducibility of our experiments.\n",
    "\n",
    "Steps:\n",
    "- Specify the dataset name to retrieve\n",
    "- Fetch the latest version of the registered dataset from the Azure ML workspace\n",
    "- Confirm successful retrieval by displaying dataset name and version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = f\"rsna_pneumonia_dataset-{unique_name}\"\n",
    "\n",
    "# Retrieve the registered dataset\n",
    "registered_data = ml_client.data.get(name=data_name, label=\"latest\")\n",
    "print(f\"Retrieved dataset: {registered_data.name}, version: {registered_data.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring and Submitting the Training Job\n",
    "\n",
    "This cell represents the culmination of our workflow setup, where we define and launch the actual machine learning training job in Azure ML. By configuring a fully parameterized job definition, we create a reproducible, scalable, and tracked training process that can run on cloud infrastructure without manual intervention.\n",
    "\n",
    "The job configuration brings together all previously established components:\n",
    "- **Input Data**: References our registered pneumonia dataset\n",
    "- **Hyperparameters**: Defines training parameters that control model behavior\n",
    "- **Compute Resources**: Utilizes our GPU cluster for accelerated training\n",
    "- **Environment**: Uses our custom PyTorch environment with all dependencies\n",
    "- **Code Assets**: Points to our training script within the source directory\n",
    "- **Output Management**: Configures MLflow tracking for experiment monitoring\n",
    "\n",
    "This approach offers several critical advantages:\n",
    "- **Reproducibility**: Every aspect of the training process is defined and versioned\n",
    "- **Scalability**: Jobs can be executed on powerful cloud infrastructure\n",
    "- **Separation of Concerns**: Code, data, compute, and parameters are managed independently\n",
    "- **Experiment Tracking**: Integration with MLflow captures metrics and artifacts automatically\n",
    "- **Workflow Automation**: The job can be incorporated into larger ML pipelines\n",
    "\n",
    "Steps:\n",
    "- Define job inputs including dataset path and hyperparameters\n",
    "- Configure a command job with:\n",
    "  - Source code location\n",
    "  - Execution command with parameterized inputs\n",
    "  - Compute target configuration\n",
    "  - Environment specification\n",
    "  - Experiment organization details\n",
    "- Submit the job to Azure ML for execution\n",
    "- Return the job object for tracking and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Define job inputs including dataset and training hyperparameters\n",
    "inputs = {\n",
    "    \"data_dir\": Input(type=AssetTypes.URI_FOLDER, path=registered_data.path),  # Reference to pneumonia dataset\n",
    "    \"max_epochs\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    \"mlflow_model_dir\": \"outputs/mlflow_model_dir\"  # Location to store MLflow model\n",
    "    }\n",
    "\n",
    "# Configure the training job with code, compute, and environment\n",
    "job = command(\n",
    "    code=\"./src\",  # Source code directory containing training script\n",
    "    command=\"python train.py --data_dir ${{inputs.data_dir}} --max_epochs ${{inputs.max_epochs}} --learning_rate ${{inputs.learning_rate}} --batch_size ${{inputs.batch_size}} --mlflow_model_dir ${{inputs.mlflow_model_dir}}\",\n",
    "    compute=gpu_cluster_name,  # Use the GPU cluster defined earlier\n",
    "    environment=registered_env,  # Use PyTorch GPU environment with dependencies\n",
    "    inputs=inputs,\n",
    "    experiment_name=f\"workshop-{unique_name}\",  # For organizing related jobs in AzureML\n",
    "    display_name=\"test_job\"  # Human-readable name in the AzureML UI\n",
    ")\n",
    "\n",
    "print(\"Job created with dataset input. Ready to submit.\")\n",
    "# Submit the job to AzureML and get a reference to the running job\n",
    "test_job = ml_client.jobs.create_or_update(job)\n",
    "test_job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Output Streaming\n",
    "\n",
    "This cell connects to the running training job and displays its logs in real-time, allowing us to monitor progress, track metrics, and detect any issues as they occur without leaving the notebook environment.\n",
    "\n",
    "**_Note_: This cell will not complete until the job finishes execution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(test_job.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Sweep Job\n",
    "\n",
    "This cell configures and launches an automated hyperparameter tuning experiment that systematically explores different learning rate values to optimize model performance. By leveraging Azure ML's sweep capabilities, we can efficiently discover optimal hyperparameters without manually testing each configuration.\n",
    "\n",
    "### Parameter Sweeps\n",
    "Parameter sweeps are essential in machine learning development for several key reasons:\n",
    "- **Objective optimization**: Automatically identifies parameter combinations that maximize model performance\n",
    "- **Efficient exploration**: LogUniform sampling efficiently explores values across orders of magnitude (10^-10 to 10^-2)\n",
    "- **Reduced human bias**: Systematic exploration may discover unintuitive but effective parameters humans might not try\n",
    "- **Time savings**: Parallel trials dramatically reduce the time needed to find optimal configurations\n",
    "- **Reproducibility**: The structured approach ensures the parameter search process is documented and repeatable\n",
    "\n",
    "In this specific sweep, we're exploring learning rate values, one of the most critical hyperparameters that influences convergence speed and final model quality.\n",
    "\n",
    "### Compute Clusters with AzureML\n",
    "Our sweep job leverages the GPU compute cluster we configured earlier, which provides several advantages:\n",
    "- **Resource elasticity**: Scales between 0-4 nodes as needed, maximizing resource utilization\n",
    "- **Parallel execution**: Runs multiple trials simultaneously, accelerating the optimization process\n",
    "- **Cost management**: Auto-scaling ensures we only pay for compute when actively using it\n",
    "- **Hardware optimization**: GPU acceleration dramatically speeds up each individual training run\n",
    "- **Centralized management**: All compute resources are managed through the AzureML platform\n",
    "\n",
    "Steps:\n",
    "- Create a new job configuration with a learning rate sweep parameter (LogUniform distribution from 10^-10 to 10^-2)\n",
    "- Configure sweep settings:\n",
    "  - Random sampling strategy to explore parameter space\n",
    "  - Maximum of 12 total trials with 3 running concurrently\n",
    "  - Early termination policy (Bandit) to automatically stop underperforming trials\n",
    "  - Target metric set to \"val_best_metric_val\" for maximization\n",
    "- Submit the sweep job for execution on our GPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import sweep\n",
    "\n",
    "command_for_sweep_job = job(learning_rate=sweep.LogUniform(-10, -2), max_epochs=20, batch_size=sweep.Choice([16, 32, 48]))\n",
    "command_for_sweep_job.display_name = None\n",
    "command_for_sweep_job.experiment_name = job.experiment_name\n",
    "sweep_job = command_for_sweep_job.sweep(\n",
    "    sampling_algorithm=\"random\",\n",
    "    primary_metric=\"val_goal_metric_val\",\n",
    "    goal=\"Maximize\",\n",
    "    max_total_trials=24,\n",
    "    max_concurrent_trials=3,\n",
    "    early_termination_policy=sweep.BanditPolicy(\n",
    "        slack_factor=0.15, evaluation_interval=1, delay_evaluation=3\n",
    "    ))\n",
    "\n",
    "ml_client.jobs.create_or_update(sweep_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
